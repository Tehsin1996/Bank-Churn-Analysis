# -*- coding: utf-8 -*-
"""New_Version_Churn_Modeling_Team6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10S8FGCLNOgPoOFF5YSRrv6a3Mcx6lB1B
"""

from google.colab import auth
auth.authenticate_user()

from pydrive.drive import GoogleDrive
from pydrive.auth import GoogleAuth
from oauth2client.client import GoogleCredentials
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

myfile = drive.CreateFile({'id': '19n2lvsNTEdddK0dqEx9woX3BhMKXYn5v'})
myfile.GetContentFile('file.csv')

# import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.naive_bayes import GaussianNB
from scipy import stats
# read in data set
df = pd.read_csv('file.csv')
df.head()

df.head()
df.shape
df.info()
df.describe()
df.head()
for col in df:
    print(col)
df.describe()

"""**Correlation Analysis**"""

# correlation matrix
df.corr()

# Correlation Heatmap
ax=sns.heatmap(df.corr(),annot=True,fmt='.1g',cmap='coolwarm')
ax.get_ylim()
ax.set_ylim(11.0,-2.0)

corr = df.corr()
corr.style.background_gradient(cmap='coolwarm')

"""**Outlier Analysis**"""

# Show Maxplot
sns.boxplot(x=df['CreditScore'])

sns.boxplot(x=df['Age'])

sns.boxplot(df['Tenure'])

sns.boxplot(df['Balance'])

sns.boxplot(df['EstimatedSalary'])

sns.boxplot(x=df['NumOfProducts'])

"""**Put multiple boxplots in one graph**"""

# put multiple boxplots in one graph
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
numeric_df=df[['CreditScore','Age','Tenure','Balance','EstimatedSalary','NumOfProducts']]
numeric_scaled=pd.DataFrame(scaler.fit_transform(numeric_df),columns=numeric_df.columns)
numeric_scaled
import matplotlib.pyplot as plt
for column in numeric_scaled:
     plt.figure()
     df.boxplot([column])
numeric_scaled.plot(kind='box')

"""**Z-score method to detect and remove outliers**

*   List item
*   List item
"""

numeric_df=df[['CreditScore','Age','Tenure','Balance','EstimatedSalary','NumOfProducts']]
from scipy import stats
import numpy as np
z=np.abs(stats.zscore(numeric_df))
print(z)
threshold=3
print(np.where(z>3))

# remove outliers
df_nooutlier=df[(z<3).all(axis=1)]
print(df.shape)
print(df_nooutlier.shape)

"""**Visualizing with respect to Target Variable**"""

sns.countplot(x='Geography', hue = 'Exited',data = df_nooutlier)

sns.countplot(x='Gender', hue = 'Exited',data = df_nooutlier)

sns.countplot(x='HasCrCard', hue = 'Exited',data = df_nooutlier)

sns.countplot(x='IsActiveMember', hue = 'Exited',data = df_nooutlier)

sns.countplot(x='NumOfProducts', hue = 'Exited',data = df_nooutlier)

sns.countplot(x='Tenure', hue = 'Exited',data = df_nooutlier)

plt.figure(figsize=(20, 30))
sns.countplot(x='Age', hue = 'Exited', data = df_nooutlier)

sns.catplot(x="Geography", y="CreditScore", kind="box", data= df_nooutlier)

sns.catplot(x="Geography", y="CreditScore", hue="Exited", kind="box", data=df_nooutlier)

df_nooutlier.drop(['RowNumber','Surname'],axis=1,inplace=True)

df_nooutlier.head()

df_nooutlier.tail()

"""**Visualizing with multiple variables**"""

# set different data types apart
df.drop(['RowNumber','Surname'],axis=1,inplace=True)

df.drop(['CustomerId'],axis=1,inplace=True)

categorical = [var for var in df.columns if df[var].dtypes == 'object']
categorical

# list numerical variable
numerical = [var for var in df.columns if df[var].dtypes != 'object']
numerical

# Distribution plots for all numeric variables.
from scipy import stats
fig = plt.figure()
i = 0
for col in numerical:
    try:
        ax = fig.add_subplot(2,4,i+1)
        sns.distplot(df[col], kde=True, fit=stats.norm)
    except ValueError:
        print(str(i)+ ' '+ 'ValueError')
    except RuntimeError:
        print(str(i)+ ' '+ 'RuntimeError')
    i = i+1
fig.set_size_inches(10,10) 
# our features are almost normal distribution

# bar plots of all categoricals
fig = plt.figure()
plt.figure(figsize=(30,30))
i = 1
for col in categorical:
    ax = fig.add_subplot(1,2,i)
    df[col].value_counts().plot(kind = 'bar', ax = ax).set_title(col) 
    i = i+1
fig.set_size_inches(6, 6)

# Bar plots of all categorical variables related to Y
fig = plt.figure()
plt.figure(figsize=(30,30))
i = 1
for col in categorical:
    ax = fig.add_subplot(1,2,i)
    sns.countplot(x=col,hue='Exited',data=df,ax = ax).set_title(col) 
    i = i+1
fig.set_size_inches(6, 6)

# Boxplots for all numeric variables to Y
fig = plt.figure()
plt.figure(figsize=(120,120))
i = 1
for col in numerical:
    ax = fig.add_subplot(2,5,i)
    sns.boxplot(x = 'Exited', y = col, data = df, ax = ax).set_title(col) 
    i = i+1
fig.set_size_inches(20, 20)

# Bar plot for all numerical variables to Y
fig = plt.figure()
plt.figure(figsize=(120,60))
i = 1
for col in numerical:
    ax = fig.add_subplot(2,5,i)
    df.groupby(['Exited'])[col].mean().plot(kind = 'bar', ax = ax).set_title(col) 
    i = i+1
fig.set_size_inches(12, 12)

"""**Pre-Processing, Sampling**"""

# Pre-Processing
df_nooutlier.head()
X=df_nooutlier[['CreditScore','Geography','Gender','Age','Tenure','Balance','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary']].values
y=df_nooutlier["Exited"]

##Sklearn do not handle categorical variables
from sklearn import preprocessing
le_Geography = preprocessing.LabelEncoder()
le_Geography.fit(['France','Spain','Germany'])
X[:,1] = le_Geography.transform(X[:,1]) 

le_Gender = preprocessing.LabelEncoder()
le_Gender.fit([ 'Male','Female'])
X[:,2] = le_Gender.transform(X[:,2])

X[0:5]

# Seperate data into training and test datasets
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)

# Standardize data
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
X_train_std[0:5]

"""**Modeling**

**1. Perceptron**
"""

# Train a perceptron model
from sklearn.linear_model import Perceptron
ppn = Perceptron(max_iter=40,eta0=0.1,random_state=0)
ppn.fit(X_train_std,y_train)
# Make predictions
y_pred = ppn.predict(X_test_std)
y_pred
# Evaluation
print("Misclassified samples: %d" % (y_test != y_pred).sum())
len(y_test)
from sklearn.metrics import accuracy_score
print("Accuracy of Preceptron Model: ", round(accuracy_score(y_test,y_pred)*100,1),'%')

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_test,y_pred))

# ROC-AUC Curve for perceptron Model
fpr, tpr, thresholds_RF = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)
plt.title('Receiver Operating Characteristic of Preceptron Model')
plt.plot(fpr, tpr, 'b',label='AUC curve (area = %0.2f)'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""**2. Decision Tree**"""

# Import Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier 
# Create Decision Tree classifer object
clf2 = DecisionTreeClassifier(criterion="entropy", max_depth = 4)
#Predict the response for test dataset
clf2.fit(X_train_std,y_train)
#Predict the response for test dataset
y_pred = clf2.predict(X_test_std)
#feature_importances
df = pd.DataFrame(data = clf2.feature_importances_, index=['CreditScore','Geography','Gender','Age','Tenure','Balance','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary'], columns=['importance'])
feature_importances = df.sort_values('importance',ascending=False)
feature_importances

clf2

feature_cols=df_nooutlier[['CreditScore','Geography','Gender','Age','Tenure','Balance','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary']].columns

# Visualize decision tree
from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO  
from IPython.display import Image  
import pydotplus

dot_data = StringIO()
export_graphviz(clf2, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True,feature_names = feature_cols,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('ChurnTree.png')
Image(graph.create_png())

# Evaluation
from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test, y_pred))

#Accuracy
print("Accuracy of decision tree: ", round(clf2.score(X_test_std, y_test)*100,2),'%')

# ROC-AUC Curve for decision tree Model
fpr, tpr, thresholds_RF = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)
plt.title('Receiver Operating Characteristic of decision tree')
plt.plot(fpr, tpr, 'b',label='AUC curve (area = %0.2f)'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""**3. Random Forest**"""

#Import Random Forest Model
from sklearn.ensemble import RandomForestClassifier
#Create a Gaussian Classifier
clf=RandomForestClassifier(n_estimators=100)
#Train the model using the training sets y_pred=clf.predict(X_test)
clf.fit(X_train_std,y_train)
y_pred=clf.predict(X_test_std)

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics


#feature_importances
df = pd.DataFrame(data = clf.feature_importances_, index=['CreditScore','Geography','Gender','Age','Tenure','Balance','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary'], columns=['importance'])
feature_importances = df.sort_values('importance',ascending=False)
feature_importances

# Evaluation
print(classification_report(y_test,y_pred))

# Evaluation
print("Misclassified samples: %d" % (y_test != y_pred).sum())
misclassified_samples = (y_test != y_pred).sum()
print("Misclassification rate: ", (misclassified_samples / len(y_pred)))

print(confusion_matrix(y_test, y_pred))

#Accuracy
print("Accuracy of random forest" ,round(clf.score(X_test_std, y_test)*100,2),'%')

# ROC-AUC Curve for random forest Model
fpr, tpr, thresholds_RF = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)
plt.title('Receiver Operating Characteristic of random forest')
plt.plot(fpr, tpr, 'b',label='AUC curve (area = %0.2f)'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""**4. Neural Network**"""

# Commented out IPython magic to ensure Python compatibility.
# Matt deep neural network

# %tensorflow_version 1.x
from tensorflow.keras import models
from tensorflow.keras.layers import Dense
model = models.Sequential()
model.add(Dense(16, input_dim = 10, activation = 'relu'))
model.add(Dense(12, activation = 'relu'))
model.add(Dense(8, activation = 'relu'))
model.add(Dense(1, activation = 'sigmoid'))
model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
model.fit(X_train_std,y_train,epochs = 10, batch_size = 20)

from sklearn.metrics import accuracy_score
print("Accuracy of DNNs:", round(accuracy_score(y_test,y_pred)*100,2),'%')
#model.add(Dense(10, input_dim=))

test_loss, test_acc = model.evaluate(X_test_std, y_test)
print("Test Accuracy: %.2f" % test_acc)

print(classification_report(y_test,y_pred))

# ROC-AUC Curve for neural networks Model
fpr, tpr, thresholds_RF = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)
plt.title('Receiver Operating Characteristic of DNNs')
plt.plot(fpr, tpr, 'b',label='AUC curve (area = %0.2f)'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""**5. Logistic Regression**"""

#Predict function
# Prediction Function to fit and make predictions
def predct_func(classification_models, features, comparison):
    ''' Here we are going to fit our train data by passing 
        to the funtion predct_func and predict our X_test 
        data which is our new test cases here and it will result 
        out classification-report and all the error metrics of the
        particular model used here.
    ''' 
    # Lets fit the model first
    classification_models.fit(features, comparison)
    # Predict new test cases
    predicted_vals = classification_models.predict(X_test)
    # Lets apply K-Fold Croos Validatiion CV=10
    KVC = cross_val_score(estimator=classification_models, X=features, y=comparison,cv=10)
    KFoldCross_Accuracies = KVC.mean()
    print('K Fold Crossvalidation Accuracy------->', KFoldCross_Accuracies)
    print()
    # Generates the classification report of the model
    print("************Classification Report*************")
    print()
    class_report = classification_report(y_test,predicted_vals)
    print(class_report)
    # Generate the Confusion Matrix of the Model
    print("************Confusion Matrix*******************")
    print()
    CM = confusion_matrix(y_test, predicted_vals)
    print(CM)

# Define another function for Evaluation of out models
def eval_model(actual_vals, prediction_vals):
    ''' Function for evaluation of error metrics
        generates confusion matrix and results out
        False Positive Rate, False Negative Rate, 
        Sensitivity/TruePositiveRate/Recall & 
        specificity/TrueNegativeRate of models
    '''
    
    CM = pd.crosstab(actual_vals, prediction_vals)
    TN = CM.iloc[0,0]
    FN = CM.iloc[1,0]
    TP = CM.iloc[1,1]
    FP = CM.iloc[0,1]
    print()
    
    # Error Metrics of the model algorithms
    print("<---------------ERROR METRICS-------------->")
    print()
    # False Negative Rate
    print("False Negative Rate-------------->",  (FN*100)/(FN+TP))
    print()
    # False Positive Rate
    print("False Positive Rate-------------->",  (FP*100)/(FP+TN))
    print()
    # Sensitivity
    print("Sensitivity/TPR/Recall----------->",  (TP*100)/(TP+FN))
    print()
    # Specificity
    print("Specificity/TNR------------------>",  (TN*100)/(TN+FP))

# Lets Develop Logistic Regression Model
LR_Model = LogisticRegression()
predct_func(LR_Model, X_train_std,y_train)

# Predict New Test Cases
LR_Predictions = LR_Model.predict(X_test_std)

# evaluate Error Metrics For Logistic Regression Model
eval_model(y_test, LR_Predictions)

# ROC-AUC Curve for Logistic Regression Model
fpr, tpr, thresholds_RF = roc_curve(y_test, LR_Predictions)
roc_auc = auc(fpr, tpr)
plt.title('Receiver Operating Characteristic of logistic regression')
plt.plot(fpr, tpr, 'b',label='AUC curve (area = %0.2f)'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""**6. Naive Bayes**"""

# Develop Naive Bayes Model
NB_Model  = GaussianNB()
predct_func(NB_Model, X_train_std,y_train)

# Predict new test cases
NB_Predictions = NB_Model.predict(X_test_std)

# evaluate Error Metrics For Naive Bayes Model
eval_model(y_test, NB_Predictions)

# ROC-AUC Curve for Naive Bayes Model
fpr, tpr, thresholds_RF = roc_curve(y_test, NB_Predictions)
roc_auc = auc(fpr, tpr)
plt.title('Receiver Operating Characteristic of Naive Byes Model')
plt.plot(fpr, tpr, 'b',label='AUC curve (area = %0.2f)'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""**7. K-Nearest Neighbors**"""

# K-Nearset Neighbor
# Import Library
from sklearn.neighbors import KNeighborsClassifier
# Training at certain k
k = 4
#Train Model and Predict  
neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train_std,y_train)
neigh
# Predicting
yhat = neigh.predict(X_test_std)
yhat[0:5]
# Accuracy Evaluation
from sklearn import metrics
print("Accuracy of KNN (K=4): ", round(metrics.accuracy_score(y_test, neigh.predict(X_test_std))*100,2),'%')

# Find the best K
Ks = 10
mean_acc = np.zeros((Ks-1))
std_acc = np.zeros((Ks-1))
ConfustionMx = [];
for n in range(1,Ks):
    
    #Train Model and Predict  
    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train_std,y_train)
    yhat=neigh.predict(X_test_std)
    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)
    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])
    
mean_acc

# Plot model accuracy for different number of neighbors
plt.plot(range(1,Ks),mean_acc,'g')
plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
plt.legend(('Accuracy ', '+/- 3xstd'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Nabors (K)')
plt.title('model accuracy with the change of K')
plt.tight_layout()
plt.show()

# the best k=8
k = 8
#Train Model and Predict  
neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train_std,y_train)
neigh
# Predicting
yhat = neigh.predict(X_test_std)
yhat[0:5]
# Accuracy Evaluation
from sklearn import metrics
print("Accuracy of KNN (K=8): ", round(metrics.accuracy_score(y_test, neigh.predict(X_test_std))*100,2),'%')
predct_func(neigh, X_train_std,y_train)

# ROC-AUC Curve for k-NN Model
fpr, tpr, thresholds_RF = metrics.roc_curve(y_test, y_pred)
roc_auc = metrics.auc(fpr, tpr)
plt.title('Receiver Operating Characteristic of k-NN')
plt.plot(fpr, tpr, 'b',label='AUC curve (area = %0.2f)'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""**8. Support Vector Machine**"""

# Support functions
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from scipy.stats import uniform

# Fit models
from sklearn.svm import SVC
from xgboost import XGBClassifier

# Scoring functions
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

# SVM with RBF Kernel
SVM_RBF = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf', max_iter=-1, probability=True, 
              random_state=None, shrinking=True,tol=0.001, verbose=False)
SVM_RBF.fit(X_train_std,y_train)

print(classification_report(y_train,  SVM_RBF.predict(X_train_std)))

# SVM with Poly Kernel
SVM_POL = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,  decision_function_shape='ovr', degree=2, gamma=0.1, kernel='poly',  max_iter=-1,
              probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False)
SVM_POL.fit(X_train_std,y_train)

print(classification_report(y_train,  SVM_POL.predict(X_train_std)))

# Test model prediction accuracy on test data
# SVM with RBF Kernel
print(classification_report(y_test,  SVM_RBF.predict(X_test_std)))
# SVM with Pol Kernel
print(classification_report(y_test,  SVM_POL.predict(X_test_std)))

# ROC-AUC Curve for SVM RBF Model
fpr, tpr, thresholds_RF = roc_curve(y_test,  SVM_RBF.predict(X_test_std))
roc_auc = auc(fpr, tpr)
plt.title('Receiver Operating Characteristic of SVM RBF Model')
plt.plot(fpr, tpr, 'b',label='AUC curve (area = %0.2f)'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

# ROC-AUC Curve for SVM Poly Model
fpr, tpr, thresholds_RF = roc_curve(y_test,  SVM_POL.predict(X_test_std))
roc_auc = auc(fpr, tpr)
plt.title('Receiver Operating Characteristic of SVM POL Model')
plt.plot(fpr, tpr, 'b',label='AUC curve (area = %0.2f)'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""**9. Extreme Gradient Boost Model**"""

# Extreme Gradient Boost Classifier

XGB = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bytree=1, gamma=0.01, learning_rate=0.1, max_delta_step=0,max_depth=7,
                    min_child_weight=5, missing=None, n_estimators=20,n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,reg_alpha=0, 
                    reg_lambda=1, scale_pos_weight=1, seed=None, silent=True, subsample=1)
XGB.fit(X_train_std, y_train)

# Classification Report
print(classification_report(y_train,  XGB.predict(X_train_std)))

# ROC-AUC Curve for Extreme Gradient Boost Model
fpr, tpr, thresholds_RF = roc_curve(y_test,  XGB.predict(X_test_std))
roc_auc = auc(fpr, tpr)
plt.title('Receiver Operating Characteristic Extreme Gradient Boost Model')
plt.plot(fpr, tpr, 'b',label='AUC curve (area = %0.2f)'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""**Report accuracy of different mdoels**"""

from prettytable import PrettyTable
t = PrettyTable(['Model', 'Accuracy'])
t.add_row(['Preceptron Model',round(accuracy_score(y_test,y_pred),4)])
t.add_row(['Decision Tree',round(clf2.score(X_test, y_test),4)])
t.add_row(['random forest',round(clf.score(X_test, y_test),4)])
t.add_row(['DNNs', round(accuracy_score(y_test,y_pred),4)])
t.add_row(['KNN (best K=8)', round(metrics.accuracy_score(y_test, neigh.predict(X_test_std)),4)])
t.add_row(['logistic regression',round(accuracy_score(y_test, LR_Predictions),4)])
t.add_row(['Naive Bayes',round(accuracy_score(y_test, NB_Predictions),4)])
t.add_row(['SVM_RBF Model',round(accuracy_score(y_test,SVM_RBF.predict(X_test_std)),4)])
t.add_row(['SVM_POL Model',round(accuracy_score(y_test,SVM_POL.predict(X_test_std)),4)])
t.add_row(['Extreme Gradient Boost',round(accuracy_score(y_test,XGB.predict(X_test_std)),4)])
print(t)

"""**Plot multiple ROC Curves in one plot**"""

# Add the models to the list that you want to view on the ROC plot
models = [
{
    'label': 'KNN',
    'model': KNeighborsClassifier(n_neighbors = 8),
},
{
    'label': 'Extreme Gradient Boosting',
    'model': XGBClassifier(),
},
{
    'label':'Decision Tree',
    'model':DecisionTreeClassifier(criterion="entropy", max_depth = 4),
},
{
    'label':'Random Forest',
    'model':RandomForestClassifier(n_estimators=100),
 
},
{
    'label':'Logistic Regression',
    'model':LogisticRegression(),
},
{
    'label':'Naive Bayes',
    'model':GaussianNB(),
},
{
    'label':'SVM with RBF Kernel',
    'model':SVC(C=100, gamma=0.1, kernel='rbf', probability=True),
},
{
    'label':'SVM with Poly Kernel',
    'model':SVC(C=100, gamma=0.1, kernel='poly', probability=True),
},
]
# Below for loop iterates through your models list
fig = plt.figure(figsize=(8,6))
for m in models:
    model = m['model'] # select the model
    model.fit(X_train_std, y_train) # train the model
    y_pred=model.predict(X_test_std) # predict the test data
# Compute False postive rate, and True positive rate
    fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test_std)[:,1])
# Calculate Area under the curve to display on the plot
    auc = metrics.roc_auc_score(y_test,model.predict(X_test_std))
# Now, plot the computed values
    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['label'], auc))
# Custom settings for the plot 

plt.plot([0, 1], [0, 1],'r--')

plt.xticks(np.arange(0.0, 1.1, step=0.1))
plt.xlabel("Flase Positive Rate", fontsize=15)

plt.yticks(np.arange(0.0, 1.1, step=0.1))
plt.ylabel("True Positive Rate", fontsize=15)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)
plt.legend(loc='lower right')
plt.show()   # Display

"""Proportion of retained and exited customers in the training dataset"""

# Looking at the proportions of customers exited and retained in the training sample
print(y_train.value_counts())
y_train.value_counts().plot(kind='bar')
plt.ylabel('Count')
plt.title('Class Counts (Checking for Imbalance)')

print("There are 20% of the customers who have churned and 80% of the customers who are retained")

"""Clearly, there is a target class imbalance problem in this scenario

**Random Oversampling:** 
Random oversampling involves randomly selecting examples from the minority 
class (exited = 1 in this case), with replacement, and adding them to training sample.
"""

# Random oversampling to rectify the target class imbalance problem

from imblearn import over_sampling
from imblearn.over_sampling import RandomOverSampler

sampler = RandomOverSampler()
X_train_sampled, y_train_sampled = sampler.fit_sample(X_train, y_train)
y_train_sampled = pd.Series(y_train_sampled)
print(y_train_sampled.value_counts())

y_train_sampled.value_counts().plot(kind='bar')
plt.ylabel('Count')
plt.title('Class Counts (Checking for Imbalance)')

X_train_sampled

#Import Random Forest Model
from sklearn.ensemble import RandomForestClassifier
#Create a Gaussian Classifier
clf=RandomForestClassifier(n_estimators=100)
#Train the model using the training sets y_pred=clf.predict(X_test)
clf.fit(X_train_sampled,y_train_sampled)
y_pred=clf.predict(X_test)

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

"""**Evaluation**"""

# Evaluation
print(classification_report(y_test,y_pred))

# Evaluation
print("Misclassified samples: %d" % (y_test != y_pred).sum())
misclassified_samples = (y_test != y_pred).sum()
print("Misclassification rate: ", (misclassified_samples / len(y_pred)))

print(confusion_matrix(y_test, y_pred))

#Accuracy
print("Accuracy of random forest" ,round(clf.score(X_test, y_test)*100,2),'%')

# ROC-AUC Curve for random forest Model
fpr, tpr, thresholds_RF = metrics.roc_curve(y_test, y_pred)
roc_auc = metrics.auc(fpr, tpr)
plt.title('Receiver Operating Characteristic of random forest')
plt.plot(fpr, tpr, 'b',label='AUC curve (area = %0.2f)'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()



"""**Summary**

According to the accuracy of models, the best performance model is random forest and the worst performance model is Naive Bayes.
"""

# Changing the variable types for test and predicted data 
y_pred = pd.DataFrame(y_pred)

y_test = pd.DataFrame(y_test)

X_test = pd.DataFrame(X_test)

y_pred.reset_index(drop=True, inplace=True)
y_test.reset_index(drop=True, inplace=True)
X_test.reset_index(drop=True, inplace=True)

# Reassigning column names
X_test.columns = ['CreditScore','Geography','Gender','Age','Tenure','Balance','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary']
y_test.columns = ["exited_actuals"]
y_pred.columns = ["exited_predicted"]

# concatenating as a dataframe
dataForInsights = pd.concat((X_test,y_pred,y_test),axis = 1)
print(dataForInsights.head())
print(dataForInsights.shape)

insightData = dataForInsights.loc[dataForInsights['exited_predicted'] == 1]
insightData.mean()